{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import wget\n",
    "import requests as req\n",
    "from io import BytesIO\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Reshape,Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовительный этап\n",
    "Полагаем, что изначально имеем папки с квадратными изображениями одного размера соответствующих классов (две папки - два класса).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В противном случае выполним преобразование:  \n",
    "<br>\n",
    "`for file_name in os.listdir(<путь до сырых картинок>):\n",
    "    im = Image.open(<путь до сырых картинок> + file_name)\n",
    "    square_im = crop_max_square(im)\n",
    "    scaled_im = square_im.resize(img_size)\n",
    "    scaled_im.save(\n",
    "        <путь до преобразованных картинок> + file_name)`\n",
    "        \n",
    "`crop_max_square` описано ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Глобальные константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=(250, 250, 3) # используемый размер изображения\n",
    "batch_size=25 # размер батча - во всех моделях один и тот же\n",
    "max_epochs=100 # макс. кол-во эпох\n",
    "classes=['Ilya','Solovey'] # список классов\n",
    "\n",
    "main_path='C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/' # директория папки проекта\n",
    "#main_path='/content/gdrive/My Drive/CNN_Ilya_Solovey/' # директория папки проекта\n",
    "save_dir=[main_path+'model_1/',main_path+'model_2/',main_path+'model_3/'] # адреса сохранения моделей\n",
    "saved_name=['best_cnn.h5','best_cnn.h5','best_cnn.h5'] # имя соотв. сохр. модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фукнция обрезки изображения\n",
    "Изображение обрезается до центрального наибольшего квадрата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_max_square(pil_img):\n",
    "    img_width, img_height = pil_img.size\n",
    "    min_size = min(img_width, img_height)\n",
    "    return pil_img.crop(((img_width - min_size) // 2,\n",
    "                         (img_height - min_size) // 2,\n",
    "                         (img_width + min_size) // 2,\n",
    "                         (img_height + min_size) // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции переименования изображений и получения названия их класса\n",
    "Изображения из исходных папок сохраняются в общую папку с уникальным именем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_n_move(direc_old,direc_new,keyword):\n",
    "    folder = os.listdir(direc_old)\n",
    "    for i, file_name in enumerate(folder): \n",
    "        file_oldname = os.path.join(direc_old, file_name)\n",
    "        point_pos=file_oldname[::-1].find('.')\n",
    "        im = Image.open(direc_old + file_name)\n",
    "        im.save(direc_new+keyword+'_'+str(i)+file_oldname[-(point_pos+1):])\n",
    "            \n",
    "def get_fname_class(arr_data, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for filename in files:\n",
    "            key_pos=filename.find('_')\n",
    "            arr_data.append([filename, filename[:key_pos]])    \n",
    "    return arr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция создания каталога с двумя подкаталогами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(dir_name):\n",
    "    if os.path.exists(dir_name):\n",
    "        shutil.rmtree(dir_name)\n",
    "    os.makedirs(dir_name)\n",
    "    os.makedirs(os.path.join(dir_name, classes[0]))\n",
    "    os.makedirs(os.path.join(dir_name, classes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция определения класса одного конкретного изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificaton_img(image,type_img):\n",
    "    x_test=np.asarray(image, dtype=np.float64)\n",
    "    x_test=x_test.reshape(1,img_size[0],img_size[1],img_size[2])\n",
    "    x_test=x_test/255\n",
    "    prediction=model_used.predict(x_test)\n",
    "    if prediction[0][0]>prediction[0][1]:\n",
    "        print('{} ({}, вер. {:.3f})'.format(classes[0],type_img,prediction[0][0]), end='')\n",
    "    elif prediction[0][0]<prediction[0][1]:\n",
    "        print('{} ({}, вер. {:.3f})'.format(classes[1],type_img,prediction[0][1]), end='')\n",
    "    else:\n",
    "        print('непонятно кто ({})'.format(type_img), end='')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_Ilya=main_path+'source_images/Ilya_Muromets/'\n",
    "img_path_Solovey=main_path+'source_images/The_Nightingale_The_Robber/'\n",
    "\n",
    "img_path_all=main_path+'all_imgs/'\n",
    "if os.path.exists(img_path_all):\n",
    "    shutil.rmtree(img_path_all)\n",
    "os.makedirs(img_path_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переносим все изображения в одну папку, переименовывая их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_n_move(img_path_Ilya,img_path_all, classes[0])\n",
    "rename_n_move(img_path_Solovey,img_path_all, classes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь все имеющиеся изображения находятся в одной папке и имеют уникальные порядковые номера.  \n",
    "Разделим их на две: в одной - изображения для обучения и валидации, в другой - для тестирования.  \n",
    "Для теста выберем 10% изображений от общего числа, а для валидации - 20% от оставшегося количества."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем списки индексов изображений для соответствующих групп."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_share=0.1\n",
    "valid_share=0.2\n",
    "total=[i for i in range(len(glob.glob(img_path_all+'*'))//len(classes))]\n",
    "\n",
    "test=random.sample(total, int(test_share*len(total)))\n",
    "\n",
    "train_n_valid=[item for item in total if item not in test]\n",
    "\n",
    "valid=random.sample(train_n_valid, int((valid_share*(1-test_share))*len(train_n_valid)))\n",
    "train=[item for item in train_n_valid if item not in valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим папки для соответствующих групп изображений и скопируем изображения по их индексам в имени в соответствующие папки.  \n",
    "Изображения разных классов копируются в разные подпадпки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path=main_path+'test/'\n",
    "train_path=main_path+'train/'\n",
    "valid_path=main_path+'valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilya\n",
      "[62, 101, 462]\n",
      "Solovey\n",
      "[62, 101, 462]\n"
     ]
    }
   ],
   "source": [
    "create_directory(test_path)\n",
    "create_directory(train_path)\n",
    "create_directory(valid_path)\n",
    "\n",
    "cnt=[0,0,0]\n",
    "for class_name in classes:\n",
    "    print(class_name)\n",
    "    cnt=[0,0,0]\n",
    "    for file_name in os.listdir(img_path_all):\n",
    "        im = Image.open(img_path_all + file_name)\n",
    "        \n",
    "        if class_name in file_name:\n",
    "            if int(re.findall('(\\d+)', file_name)[0]) in test:\n",
    "                im.save(test_path+class_name+'/'+file_name)\n",
    "                cnt[0]+=1\n",
    "            elif int(re.findall('(\\d+)', file_name)[0]) in valid:\n",
    "                im.save(valid_path+class_name+'/'+file_name)\n",
    "                cnt[1]+=1\n",
    "            else:\n",
    "                im.save(train_path+class_name+'/'+file_name)\n",
    "                cnt[2]+=1\n",
    "    else:\n",
    "        print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание наборов данных\n",
    "Создаём генераторы данных на основе изображений из соответствующих каталогов.  \n",
    "Keras автоматически определит класс изображения по названию его папки.  \n",
    "Исходные изображения для обучения и валидации могут быть случайно отражены по горизонтали и вертикали, и повёрнуты на угол до 180 град."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 924 images belonging to 2 classes.\n",
      "Found 202 images belonging to 2 classes.\n",
      "Found 124 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen_train_valid = ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=180)\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen_train_valid.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(img_size[0], img_size[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "valid_generator = datagen_train_valid.flow_from_directory(\n",
    "    valid_path,\n",
    "    target_size=(img_size[0], img_size[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = datagen_test.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(img_size[0], img_size[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание моделей и их обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Далее представлены три различных модели, самая эффективная - Модель №3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель №1\n",
    "Самая ресурсоёмкая модель и не очень точная модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_348 (Conv2D)          (None, 250, 250, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_348 (MaxPoolin (None, 125, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_349 (Conv2D)          (None, 125, 125, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_349 (MaxPoolin (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_350 (Conv2D)          (None, 62, 62, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_350 (MaxPoolin (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_351 (Conv2D)          (None, 31, 31, 128)       16512     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_351 (MaxPoolin (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 15, 15, 1651)      212979    \n",
      "_________________________________________________________________\n",
      "flatten_87 (Flatten)         (None, 371475)            0         \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 371475)            0         \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 331)               122958556 \n",
      "_________________________________________________________________\n",
      "flatten_88 (Flatten)         (None, 331)               0         \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 2)                 664       \n",
      "=================================================================\n",
      "Total params: 123,281,959\n",
      "Trainable params: 123,281,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 71s 2s/step - loss: 0.8476 - accuracy: 0.6786 - val_loss: 0.5110 - val_accuracy: 0.8762\n",
      "\n",
      "Epoch 00001: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_1\\best_cnn.h5\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 66s 2s/step - loss: 0.3397 - accuracy: 0.9372 - val_loss: 0.2394 - val_accuracy: 0.9950\n",
      "\n",
      "Epoch 00002: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_1\\best_cnn.h5\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 66s 2s/step - loss: 0.2056 - accuracy: 0.9827 - val_loss: 0.1710 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00003: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_1\\best_cnn.h5\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 66s 2s/step - loss: 0.1594 - accuracy: 0.9805 - val_loss: 0.1451 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00004: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_1\\best_cnn.h5\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 66s 2s/step - loss: 0.1234 - accuracy: 0.9903 - val_loss: 0.1224 - val_accuracy: 0.9901\n",
      "\n",
      "Epoch 00005: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_1\\best_cnn.h5\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 66s 2s/step - loss: 0.1111 - accuracy: 0.9968 - val_loss: 0.1074 - val_accuracy: 0.9950\n",
      "\n",
      "Epoch 00006: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_1\\best_cnn.h5\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 66s 2s/step - loss: 0.1038 - accuracy: 0.9913 - val_loss: 0.1085 - val_accuracy: 0.9901\n",
      "\n",
      "Epoch 00007: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_1\\best_cnn.h5\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 66s 2s/step - loss: 0.1194 - accuracy: 0.9838 - val_loss: 0.1224 - val_accuracy: 0.9950\n",
      "\n",
      "Epoch 00008: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_1\\best_cnn.h5\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 66s 2s/step - loss: 0.0837 - accuracy: 0.9968 - val_loss: 0.0858 - val_accuracy: 0.9901\n",
      "\n",
      "Epoch 00009: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_1\\best_cnn.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19cb77a7978>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = Sequential([\n",
    "    Conv2D(filters=32,kernel_size=(3,3), padding='same', strides=1, input_shape = img_size, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(filters=64,kernel_size=(3,3), padding='same', strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(filters=128,kernel_size=(3,3), padding='same', strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(filters=128,kernel_size=(1,1), padding='same', strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Dropout(0.1),\n",
    "    Dense(1651, activation='relu', activity_regularizer=regularizers.L2(0.1)),\n",
    "    Flatten(),\n",
    "    Dropout(0.25),\n",
    "    Dense(331, activation='relu', activity_regularizer=regularizers.L2(0.01)),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_1.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "earlyStop = (EarlyStopping(patience = 3, monitor=\"accuracy\", mode=\"max\"))\n",
    "\n",
    "if not os.path.exists(save_dir[0]):\n",
    "    os.mkdir(save_dir[0])\n",
    "\n",
    "checkPoint = ModelCheckpoint(save_dir[0]+saved_name[0], monitor='accuracy', mode='max', verbose=1)\n",
    "\n",
    "model_1.summary()\n",
    "\n",
    "model_1.fit(\n",
    "    train_generator,\n",
    "    #steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=max_epochs,\n",
    "    validation_data=valid_generator,\n",
    "    #validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=[earlyStop,checkPoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель №2\n",
    "Наименее ресурсоёмкая модель. Не обесечивает требуюмую точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_108\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_432 (Conv2D)          (None, 125, 125, 1)       49        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_432 (MaxPoolin (None, 62, 62, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_433 (Conv2D)          (None, 62, 62, 2)         20        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_433 (MaxPoolin (None, 31, 31, 2)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_434 (Conv2D)          (None, 31, 31, 4)         76        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_434 (MaxPoolin (None, 15, 15, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_435 (Conv2D)          (None, 15, 15, 2)         10        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_435 (MaxPoolin (None, 7, 7, 2)           0         \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 7, 7, 2)           0         \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 7, 7, 4)           12        \n",
      "_________________________________________________________________\n",
      "flatten_109 (Flatten)        (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 15s 390ms/step - loss: 0.6931 - accuracy: 0.4924 - val_loss: 0.6934 - val_accuracy: 0.4802\n",
      "\n",
      "Epoch 00001: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 14s 377ms/step - loss: 0.6931 - accuracy: 0.5108 - val_loss: 0.6937 - val_accuracy: 0.4802\n",
      "\n",
      "Epoch 00002: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 14s 377ms/step - loss: 0.6934 - accuracy: 0.5022 - val_loss: 0.6931 - val_accuracy: 0.5099\n",
      "\n",
      "Epoch 00003: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 14s 373ms/step - loss: 0.6924 - accuracy: 0.5346 - val_loss: 0.6897 - val_accuracy: 0.6139\n",
      "\n",
      "Epoch 00004: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 14s 374ms/step - loss: 0.6906 - accuracy: 0.5639 - val_loss: 0.6838 - val_accuracy: 0.6386\n",
      "\n",
      "Epoch 00005: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 14s 374ms/step - loss: 0.6710 - accuracy: 0.6353 - val_loss: 0.6219 - val_accuracy: 0.6931\n",
      "\n",
      "Epoch 00006: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 14s 377ms/step - loss: 0.6041 - accuracy: 0.6959 - val_loss: 0.5380 - val_accuracy: 0.8020\n",
      "\n",
      "Epoch 00007: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 14s 376ms/step - loss: 0.5264 - accuracy: 0.7294 - val_loss: 0.4958 - val_accuracy: 0.7921\n",
      "\n",
      "Epoch 00008: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 14s 375ms/step - loss: 0.5040 - accuracy: 0.7955 - val_loss: 0.4028 - val_accuracy: 0.8762\n",
      "\n",
      "Epoch 00009: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 14s 372ms/step - loss: 0.4611 - accuracy: 0.8063 - val_loss: 0.3736 - val_accuracy: 0.8663\n",
      "\n",
      "Epoch 00010: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 14s 373ms/step - loss: 0.3899 - accuracy: 0.8452 - val_loss: 0.3131 - val_accuracy: 0.8713\n",
      "\n",
      "Epoch 00011: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 14s 378ms/step - loss: 0.3303 - accuracy: 0.8896 - val_loss: 0.2520 - val_accuracy: 0.9158\n",
      "\n",
      "Epoch 00012: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 14s 371ms/step - loss: 0.2587 - accuracy: 0.9264 - val_loss: 0.1746 - val_accuracy: 0.9505\n",
      "\n",
      "Epoch 00013: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 14s 370ms/step - loss: 0.2013 - accuracy: 0.9394 - val_loss: 0.1487 - val_accuracy: 0.9604\n",
      "\n",
      "Epoch 00014: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 14s 377ms/step - loss: 0.1787 - accuracy: 0.9448 - val_loss: 0.1238 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00015: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 14s 388ms/step - loss: 0.1520 - accuracy: 0.9578 - val_loss: 0.1103 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00016: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 14s 392ms/step - loss: 0.1442 - accuracy: 0.9545 - val_loss: 0.1118 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00017: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 14s 390ms/step - loss: 0.1205 - accuracy: 0.9665 - val_loss: 0.0956 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00018: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 14s 377ms/step - loss: 0.1077 - accuracy: 0.9686 - val_loss: 0.0778 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00019: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 14s 373ms/step - loss: 0.1069 - accuracy: 0.9719 - val_loss: 0.0955 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00020: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 14s 376ms/step - loss: 0.0937 - accuracy: 0.9708 - val_loss: 0.0839 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00021: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 14s 377ms/step - loss: 0.0877 - accuracy: 0.9719 - val_loss: 0.0746 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00022: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 14s 370ms/step - loss: 0.0857 - accuracy: 0.9794 - val_loss: 0.0985 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00023: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 14s 370ms/step - loss: 0.0942 - accuracy: 0.9686 - val_loss: 0.0960 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00024: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 14s 379ms/step - loss: 0.0751 - accuracy: 0.9784 - val_loss: 0.0751 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00025: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 14s 389ms/step - loss: 0.0868 - accuracy: 0.9708 - val_loss: 0.0690 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00026: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 14s 371ms/step - loss: 0.0650 - accuracy: 0.9805 - val_loss: 0.0937 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00027: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 14s 373ms/step - loss: 0.0662 - accuracy: 0.9794 - val_loss: 0.0714 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00028: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 14s 383ms/step - loss: 0.0675 - accuracy: 0.9848 - val_loss: 0.0827 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00029: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 14s 381ms/step - loss: 0.0741 - accuracy: 0.9773 - val_loss: 0.0966 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00030: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 14s 389ms/step - loss: 0.0645 - accuracy: 0.9848 - val_loss: 0.1003 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00031: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 14s 382ms/step - loss: 0.0656 - accuracy: 0.9838 - val_loss: 0.1000 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00032: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 14s 379ms/step - loss: 0.0760 - accuracy: 0.9794 - val_loss: 0.0821 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00033: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 14s 381ms/step - loss: 0.0686 - accuracy: 0.9838 - val_loss: 0.0764 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00034: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 14s 382ms/step - loss: 0.0602 - accuracy: 0.9848 - val_loss: 0.0918 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00035: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 14s 385ms/step - loss: 0.0675 - accuracy: 0.9773 - val_loss: 0.0841 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00036: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 14s 373ms/step - loss: 0.0665 - accuracy: 0.9805 - val_loss: 0.0788 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00037: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_2\\best_cnn.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19cc34c4940>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = Sequential([\n",
    "    Conv2D(filters=1,kernel_size=(4,4), padding='same', strides=2, input_shape = img_size, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(filters=2,kernel_size=(3,3), padding='same', strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(filters=4,kernel_size=(3,3), padding='same', strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(filters=2,kernel_size=(1,1), padding='same', strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    \n",
    "    Dropout(0.25),\n",
    "    Dense(4, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "earlyStop = (EarlyStopping(patience = 8, monitor=\"accuracy\", mode=\"max\"))\n",
    "\n",
    "if not os.path.exists(save_dir[1]):\n",
    "    os.mkdir(save_dir[1])\n",
    "\n",
    "checkPoint = ModelCheckpoint(save_dir[1]+saved_name[1], monitor='accuracy', mode='max', verbose=1)\n",
    "\n",
    "model_2.summary()\n",
    "\n",
    "model_2.fit(\n",
    "    train_generator,\n",
    "    #steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=max_epochs,\n",
    "    validation_data=valid_generator,\n",
    "    #validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=[earlyStop,checkPoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель №3\n",
    "Самая эффективная модель - развитие модели №2 - по сравнению с предыдущими моделями требует наименьшее количество ресурсов, обеспечивая необходимую точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_121\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_484 (Conv2D)          (None, 125, 125, 1)       49        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_484 (MaxPoolin (None, 62, 62, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_485 (Conv2D)          (None, 62, 62, 2)         20        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_485 (MaxPoolin (None, 31, 31, 2)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_486 (Conv2D)          (None, 31, 31, 4)         76        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_486 (MaxPoolin (None, 15, 15, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_487 (Conv2D)          (None, 15, 15, 2)         10        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_487 (MaxPoolin (None, 7, 7, 2)           0         \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 7, 7, 2)           0         \n",
      "_________________________________________________________________\n",
      "dense_243 (Dense)            (None, 7, 7, 5)           15        \n",
      "_________________________________________________________________\n",
      "flatten_122 (Flatten)        (None, 245)               0         \n",
      "_________________________________________________________________\n",
      "dense_244 (Dense)            (None, 2)                 492       \n",
      "=================================================================\n",
      "Total params: 662\n",
      "Trainable params: 662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 15s 390ms/step - loss: 0.6880 - accuracy: 0.6234 - val_loss: 0.6793 - val_accuracy: 0.5644\n",
      "\n",
      "Epoch 00001: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 14s 380ms/step - loss: 0.6555 - accuracy: 0.6634 - val_loss: 0.6088 - val_accuracy: 0.8218\n",
      "\n",
      "Epoch 00002: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 14s 377ms/step - loss: 0.5000 - accuracy: 0.8669 - val_loss: 0.3273 - val_accuracy: 0.9356\n",
      "\n",
      "Epoch 00003: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 14s 372ms/step - loss: 0.2068 - accuracy: 0.9567 - val_loss: 0.0924 - val_accuracy: 0.9851\n",
      "\n",
      "Epoch 00004: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 14s 391ms/step - loss: 0.0901 - accuracy: 0.9794 - val_loss: 0.0636 - val_accuracy: 0.9851\n",
      "\n",
      "Epoch 00005: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 14s 384ms/step - loss: 0.0542 - accuracy: 0.9892 - val_loss: 0.0566 - val_accuracy: 0.9851\n",
      "\n",
      "Epoch 00006: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 14s 376ms/step - loss: 0.0441 - accuracy: 0.9859 - val_loss: 0.0484 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00007: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 15s 397ms/step - loss: 0.0480 - accuracy: 0.9859 - val_loss: 0.0691 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00008: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 15s 402ms/step - loss: 0.0387 - accuracy: 0.9881 - val_loss: 0.0497 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00009: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 14s 386ms/step - loss: 0.0498 - accuracy: 0.9848 - val_loss: 0.0435 - val_accuracy: 0.9851\n",
      "\n",
      "Epoch 00010: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 14s 383ms/step - loss: 0.0312 - accuracy: 0.9913 - val_loss: 0.0466 - val_accuracy: 0.9851\n",
      "\n",
      "Epoch 00011: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 15s 398ms/step - loss: 0.0397 - accuracy: 0.9870 - val_loss: 0.0631 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00012: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 14s 388ms/step - loss: 0.0356 - accuracy: 0.9892 - val_loss: 0.0646 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00013: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 14s 371ms/step - loss: 0.0319 - accuracy: 0.9892 - val_loss: 0.0738 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00014: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 14s 374ms/step - loss: 0.0335 - accuracy: 0.9881 - val_loss: 0.0401 - val_accuracy: 0.9851\n",
      "\n",
      "Epoch 00015: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 14s 387ms/step - loss: 0.0349 - accuracy: 0.9892 - val_loss: 0.0543 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00016: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 14s 373ms/step - loss: 0.0311 - accuracy: 0.9913 - val_loss: 0.0451 - val_accuracy: 0.9851\n",
      "\n",
      "Epoch 00017: saving model to C:/Users/injen/Desktop/PyProjects/ML&DS/Inf_El/LABs/CNN_Ilya_Solovey/model_3\\best_cnn.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19cc08b5470>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = Sequential([\n",
    "    Conv2D(filters=1,kernel_size=(4,4), padding='same', strides=2, input_shape = img_size, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(filters=2,kernel_size=(3,3), padding='same', strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(filters=4,kernel_size=(3,3), padding='same', strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(filters=2,kernel_size=(1,1), padding='same', strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    \n",
    "    Dropout(0.2),\n",
    "    Dense(5,activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_3.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "earlyStop = (EarlyStopping(patience = 6, monitor=\"accuracy\", mode=\"max\"))\n",
    "\n",
    "if not os.path.exists(save_dir[2]):\n",
    "    os.mkdir(save_dir[2])\n",
    "\n",
    "checkPoint = ModelCheckpoint(save_dir[2]+saved_name[2], monitor='accuracy', mode='max', verbose=1)\n",
    "\n",
    "model_3.summary()\n",
    "\n",
    "model_3.fit(\n",
    "    train_generator,\n",
    "    #steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=max_epochs,\n",
    "    validation_data=valid_generator,\n",
    "    #validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=[earlyStop,checkPoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка моделей\n",
    "Построим матрицу ошибок для каждой сохранённой модели для данных train'а и test'а."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model=[]\n",
    "for i,direc in enumerate(save_dir):\n",
    "    our_model.append(load_model(direc+saved_name[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матрицы ошибок для train'a\n",
    "Перезададим генератор для train'a - уберём элементы случайности, чтобы добиться однозначности генерации.  \n",
    "По параметрам такой генеатор идентичен генератору test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 924 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_pred_generator = datagen_test.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(img_size[0], img_size[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель №1:\n",
      "37/37 [==============================] - 12s 332ms/step - loss: 0.0958 - accuracy: 0.9892\n",
      "Модель №2:\n",
      "37/37 [==============================] - 2s 41ms/step - loss: 0.0444 - accuracy: 0.9870\n",
      "Модель №3:\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 0.0295 - accuracy: 0.9892\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_train=[]\n",
    "for i,cnn in enumerate(our_model):\n",
    "    y_pred_train.append(cnn.predict(train_pred_generator))\n",
    "    print('Модель №{}:'.format(i+1))\n",
    "    cnn.evaluate(train_pred_generator)\n",
    "y_pred_train_classes=np.argmax(y_pred_train, axis=2)\n",
    "\n",
    "cnfsn_mtrx_train=[]\n",
    "for pred_classes in y_pred_train_classes:\n",
    "    cnfsn_mtrx_train.append(confusion_matrix(train_pred_generator.classes, pred_classes))\n",
    "\n",
    "f,axes=plt.subplots(1,3,figsize=(20,5))\n",
    "for i,plot in enumerate(cnfsn_mtrx_train):\n",
    "    sns.heatmap(plot, annot=True, fmt='.0f', ax=axes[i])\n",
    "    axes[i].set_title('Model №{}'.format(i+1))\n",
    "    axes[i].set_xticklabels(classes)\n",
    "    axes[i].set_yticklabels(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матрицы ошибок для test'a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель №1:\n",
      "5/5 [==============================] - 2s 335ms/step - loss: 0.0805 - accuracy: 1.0000\n",
      "Модель №2:\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0594 - accuracy: 0.9677\n",
      "Модель №3:\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0193 - accuracy: 0.9919\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_test=[]\n",
    "for i,cnn in enumerate(our_model):\n",
    "    y_pred_test.append(cnn.predict(test_generator))\n",
    "    print('Модель №{}:'.format(i+1))\n",
    "    cnn.evaluate(test_generator)\n",
    "y_pred_test_classes=np.argmax(y_pred_test, axis=2)\n",
    "\n",
    "cnfsn_mtrx_test=[]\n",
    "for pred_classes in y_pred_test_classes:\n",
    "    cnfsn_mtrx_test.append(confusion_matrix(test_generator.classes, pred_classes))\n",
    "\n",
    "f,axes=plt.subplots(1,3,figsize=(20,5))\n",
    "for i,plot in enumerate(cnfsn_mtrx_test):\n",
    "    sns.heatmap(plot, annot=True, fmt='.0f', ax=axes[i])\n",
    "    axes[i].set_title('Model №{}'.format(i+1))\n",
    "    axes[i].set_xticklabels(classes)\n",
    "    axes[i].set_yticklabels(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка на конкретных изображениях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеем папку с изображениями исходных классов случайных размеров (скачали из интернета).\n",
    "\n",
    "НН обучалась на квадратных изображениях с нормальными пропорциями, поэтому для корректной классификации тестовых данных исходное изображение следует обрезать до наибольшего центрального квадрата и масштабировать до требуемого размера img_size.\n",
    "\n",
    "Однако, НН также успешно (но хуже) классифицирует изображения с изменёнными пропорциями (когда прямоугольник сжимают в квадрат).\n",
    "\n",
    "Следует также учитывать, что, обрезая тестовое изображение до квадрата, можно потерять часть полезных данных.\n",
    "\n",
    "Поэтому для каждого тестового изображения НН даст два ответа: для его нормального (но обрезанного) вида и для вида с изменёнными пропорциями, причём первому ответу в общем случае следует отдавать предпочтение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В папке с сырыми тестовыми изображениями создаются две папки: для квадратных и просто ресайзнутых. Поддерживаются только .jpeg, .jpg и .webp форматы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 0 # 0 - если проверяется изображение из интернета; 1 - если из директории\n",
    "\n",
    "# адресс изображения в интернете для скачивания\n",
    "url= 'https://i.pinimg.com/564x/4d/5f/71/4d5f710e6b31e7fa55052c821a1bb11f.jpg'\n",
    "\n",
    "# путь до папки с тестовыми уже скачанными сырыми изображениями\n",
    "test_path=main_path+'real_img/' # НН класиифицирует каждое изображение в этой папке\n",
    "\n",
    "if mode:\n",
    "    test_squared_path=(test_path+'squared/')\n",
    "    test_just_resized_path=(test_path+'just_resized/')\n",
    "\n",
    "    if not os.path.exists(test_squared_path):\n",
    "        os.mkdir(test_squared_path)\n",
    "    if not os.path.exists(test_just_resized_path):\n",
    "        os.mkdir(test_just_resized_path)\n",
    "\n",
    "    for file_name in os.listdir(test_path):\n",
    "        if file_name.endswith('.jpg') or file_name.endswith('.jpeg') or file_name.endswith('.webp'):\n",
    "            im = Image.open(test_path + file_name)\n",
    "\n",
    "            im_squared = crop_max_square(im)\n",
    "            scaled_im = im_squared.resize((img_size[0],img_size[1]))\n",
    "            scaled_im.save(test_squared_path + file_name)\n",
    "\n",
    "            just_resized=im.resize((img_size[0],img_size[1]))\n",
    "            just_resized.save(test_just_resized_path+file_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем модель и предсказываем класс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На изображении по URL -- Ilya (норм. проп., вер. 0.853) или Ilya (изм. проп., вер. 0.851)"
     ]
    }
   ],
   "source": [
    "model_used=our_model[2]\n",
    "\n",
    "if mode:\n",
    "    for file_name in os.listdir(test_path):\n",
    "        if file_name.endswith('.jpg') or file_name.endswith('.jpeg') or file_name.endswith('.webp'):\n",
    "            print('На изображении \"{}\" -- '.format(file_name), end='')\n",
    "\n",
    "            img_squared=Image.open(test_squared_path + file_name)\n",
    "            classificaton_img(img_squared,'норм. проп.')\n",
    "            print(' или ',end='')\n",
    "\n",
    "            img_just_resized=Image.open(test_just_resized_path + file_name)\n",
    "            classificaton_img(img_just_resized,'изм. проп.')\n",
    "            print()\n",
    "else:\n",
    "    response = req.get(url)\n",
    "    im = Image.open(BytesIO(response.content))\n",
    "    print('На изображении {} -- '.format('по URL'), end='')\n",
    "    im_squared = crop_max_square(im)\n",
    "    scaled_im = im_squared.resize((img_size[0],img_size[1]))\n",
    "    classificaton_img(scaled_im,'норм. проп.')\n",
    "    \n",
    "    print(' или ',end='')\n",
    "    \n",
    "    just_resized=im.resize((img_size[0],img_size[1]))\n",
    "    classificaton_img(just_resized,'изм. проп.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исходные материалы и актуальные модели доступны по ссылке:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://drive.google.com/drive/folders/17grYsTBmWnJSMvdK7KvpID0zheJYlxlj?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
